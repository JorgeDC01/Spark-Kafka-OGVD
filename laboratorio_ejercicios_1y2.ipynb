{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a165406d-2b10-4507-90ac-22fa41acbe55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/02 19:47:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[*]\", \"Practica1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a8da62e-911f-4801-8c0c-7cd37c5b755b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(filename):\n",
    "    text_file = sc.textFile(filename)\n",
    "    text_file = text_file.map(lambda linea: np.array(linea.split(',')).astype(float))\n",
    "    text_file= text_file.map(lambda x: (x[0:-1], int(x[-1])))\n",
    "    return text_file    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33531cd0-365e-405f-af6d-6b1da13dbdb0",
   "metadata": {},
   "source": [
    "CSV separado por comas \",\"\r\n",
    "\r\n",
    "num filas= 1.000.000, num columnas= 12\r\n",
    "\r\n",
    "Cada celda es un numero. \r\n",
    "\r\n",
    "La ultima celda de cada fila es la etiqueta (0/1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203f55b7-6a7a-4570-a9ee-c4328f27f885",
   "metadata": {},
   "source": [
    "FUNCIONES textFile, count, map, flatMap, reduce and reduceByKey. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bde680b7-f58e-4a39-955c-b43f066e477f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([ 3.54530189e+03,  3.19801395e+03,  8.00001547e+01,  1.00000192e+00,\n",
       "          4.44960086e+08,  4.76807306e+02,  1.30000000e+01, -7.71042474e-09,\n",
       "          8.70000015e+01,  1.90994302e+07,  2.46836839e+09]),\n",
       "  0),\n",
       " (array([-1.21308761e-06,  7.99998908e+00,  6.54999989e+04,  9.99999745e-01,\n",
       "          6.19998877e+01,  6.99998079e+01,  1.20000000e+01, -2.35688188e-08,\n",
       "          8.00000498e+00,  2.46836962e+09,  2.46836839e+09]),\n",
       "  1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_file= readFile('botnet_reduced_10k_l.csv')\n",
    "text_file=text_file.take(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b642061-2d1f-4248-b4ac-a700b2950ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize (RDD_Xy):\n",
    "    # ------------------------------------------------------------------------\n",
    "    rdd_rows = RDD_Xy.map(lambda row: row[0])  \n",
    "    sumas = rdd_rows.reduce(lambda x, y: x + y)  \n",
    "    means_array = sumas / RDD_Xy.count()\n",
    "    #print(f\"5 Primeras medias: {means_array[0:5]}\")\n",
    "    \n",
    "    rdd_diff = rdd_rows.map(lambda x_array: (x_array - means_array)**2)\n",
    "    array_diff_sumatorio = rdd_diff.reduce(lambda x_row, y_row: x_row + y_row)\n",
    "    array_varianza = np.maximum(array_diff_sumatorio / RDD_Xy.count(), 0)\n",
    "    #print(f\"5 Primeras Varianzas: {array_varianza[0:5]}\")\n",
    "\n",
    "    \n",
    "    # rdd_final = rdd_rows.map(lambda row: np.array([(value - means_array[idx_column]) / array_varianza[idx_column] for idx_column, value in enumerate(row)]))\n",
    "    # #rdd_final.collect())\n",
    "    # print(rdd_final.collect())\n",
    "\n",
    "    rdd_final = RDD_Xy.map(lambda row: (\n",
    "        np.array([(value - means_array[idx_column]) / np.sqrt(array_varianza[idx_column]) \n",
    "                  for idx_column, value in enumerate(row[0])]), row[1]\n",
    "    ) )\n",
    "    return rdd_final\n",
    "    #print(rdd_final.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "588d859c-c606-4051-a95c-fb7d6686d509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 1.41281668, -0.75956071, -0.41204268, -0.45940748,  1.39580934,\n",
       "         -0.35386218,  0.74057161, -0.8945781 , -0.40163083, -2.91039199,\n",
       "          0.1567098 ]),\n",
       "  0),\n",
       " (array([-0.79867306, -0.89146231,  3.64034447, -0.45940748, -0.52296869,\n",
       "         -0.35387136,  0.55184122, -0.8945781 , -1.27444226,  0.4762026 ,\n",
       "          0.1567098 ]),\n",
       "  1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1 = normalize(text_file)\n",
    "rdd1.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d96336e-06bf-4186-980e-b01097f27574",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd6= rdd1.map(lambda x : x[0])\n",
    "rdd6.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9c1d57f-4100-4833-85f0-d862bbb4f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rdd_weighted_sum = rdd.map(lambda row: sum(w * x for w, x in zip(weights, row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a05bad9-ca3e-4689-b2db-f620a34ba045",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_sigmoid(z):\n",
    "    y = (1 /(1 + np.exp(-z)))\n",
    "    return y  \n",
    "    \n",
    "def train(RDD_Xy, iterations, learning_rate, lamba_reg):\n",
    "    #inicializar pesos\n",
    "    weights= np.random.rand(11)\n",
    "    biases = np.random.rand(1)   \n",
    "    #print((weights, biases))\n",
    "\n",
    "    for it in range(iterations):\n",
    "        #número de datos\n",
    "        m=RDD_Xy.count()\n",
    "        #calcular predicción (aplicar sigmoide a la multiplicación de los pesos más el sesgi)\n",
    "        rdd2=RDD_Xy.map(lambda x: (x[0], x[1],compute_sigmoid(np.dot(x[0],weights) + biases)))\n",
    "\n",
    "        item_2_costFunction = (lamba_reg/(2*11))*np.array([w**2 for w in weights]).sum() \n",
    "        # print(f\"Calculo 2: {item_2_costFunction}\")\n",
    "        \n",
    "        rdd2 = rdd2.map(lambda x: (x[0], x[1], x[2], x[1]*np.log(x[2]) + (1 - x[1])*np.log(1-x[2]) + item_2_costFunction))\n",
    "        \n",
    "        #fórmula de la derivada\n",
    "        rdd3= rdd2.map(lambda x: ((x[2]-x[1])*x[0],(x[2]-x[1])))\n",
    "        rdd4= rdd3.reduce(lambda x,y: (x[0]+y[0], x[1]+y[1]))\n",
    "        weights2 = (weights * lamba_reg)/ len(weights)\n",
    "        d_weights= ((rdd4[0]/ m) + weights2 ) \n",
    "        d_bias= (rdd4[1]) / m\n",
    "        #actualización de las derivadas\n",
    "        weights = weights - learning_rate * d_weights\n",
    "        biases = biases - learning_rate * d_bias\n",
    "\n",
    "        rdd_cost = rdd2.map(lambda x: x[3])\n",
    "        total_cost = rdd_cost.reduce(lambda x, y: x + y)\n",
    "        cost = -(total_cost/m)\n",
    "        print(f\"Iteration [{it}]: {cost}\")\n",
    "        \n",
    "    return weights, biases\n",
    "\n",
    "train(rdd1, 10, 1.5, 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a07a831-9ca7-4e60-af8c-cfdb30601c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    y = compute_sigmoid(np.dot(X,w) + b)\n",
    "    if y >= 0.5:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "639e460c-97a4-45c2-8b23-8a125b997aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(w,b, RDD_Xy):\n",
    "    y_hat = RDD_Xy.map(lambda x: (predict(w, b, x[0]), x[1]))  \n",
    "    preds = y_hat.map(lambda x: 1 if x[0] == x[1] else 0)\n",
    "    correct_preds = preds.reduce(lambda x, y: x + y)    \n",
    "    total = RDD_Xy.count()\n",
    "    accuracy = correct_preds / total\n",
    "    \n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1610f258-d51e-4dc2-bc53-04ad0b873377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9268"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "weights, biases = train(rdd1, 10, 1.5, 0)\n",
    "acc = accuracy(weights, biases, rdd1)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9e09d72d-ac54-438c-8166-e0b4b81f9e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [0]: [0.53764832]\n",
      "Iteration [1]: [0.38397414]\n",
      "Iteration [2]: [0.31830446]\n",
      "Iteration [3]: [0.28403888]\n",
      "Iteration [4]: [0.26295742]\n",
      "Iteration [5]: [0.24851664]\n",
      "Iteration [6]: [0.23790604]\n",
      "Iteration [7]: [0.2297238]\n",
      "Iteration [8]: [0.22318947]\n",
      "Iteration [9]: [0.2178316]\n",
      "acc 0.9283\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "path = 'botnet_reduced_10k_l.csv'\n",
    "data = readFile(path)\n",
    "\n",
    "# standardize\n",
    "data = normalize(data)\n",
    "\n",
    "nIter = 10\n",
    "learningRate = 1.5\n",
    "lamba_reg = 0\n",
    "\n",
    "weights, bias = train(data, nIter, learningRate, lamba_reg)\n",
    "acc = accuracy(weights, bias, data)\n",
    "print (\"acc\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611b01f-f8f6-4bc8-891d-63c0530980ce",
   "metadata": {},
   "source": [
    "### Ejercicio 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "77594df7-7e74-46bf-b871-c46fe0fd837c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def transform(data, m, n_blocks):    \n",
    "\n",
    "    m_per_block = m / n_blocks\n",
    "    \n",
    "    indices = list(range(m)) \n",
    "    random.shuffle(indices)\n",
    "\n",
    "    indexed_data = data.map(lambda x: (1, [x]))\n",
    "    indexed_data = indexed_data.reduceByKey(lambda a, b: a + b) \n",
    "    indexed_data = indexed_data.flatMap(lambda x: [(indices[i]//m_per_block, item) for i, item in enumerate(x[1])])\n",
    "\n",
    "    return indexed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ade36261-9b65-4b93-be6b-d1563dd13b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def transform(data, m, n_blocks):   \n",
    "    #calcular n de elementos de cada bloque\n",
    "    #'//' para obtener un valor entero (división entera) sin decimales\n",
    "    m_per_block = m // n_blocks\n",
    "\n",
    "    #creamos una lista de índices que representan las filas del conjunto de datos\n",
    "    #tenemos lista tiene valores del 0 al m-1 (m es el número total de filas)\n",
    "    indices = list(range(m)) \n",
    "    # mezzclamos los índices para asegurarnos de que los datos se distribuyan de forma aleatoria entre los bloques\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    #la operación modulo % asegura que el índice de cada fila se asigna a un bloque específico.\n",
    "    # el resultado de 'indices.pop() % n_blocks' será un valor entre 0 y (n_blocks-1)\n",
    "    #el operados hace que los datos se distribuyan entre los bloques de manera UNIFORME\n",
    "    # los datos se asignen cíclicamente a bloques (0, 1, 2,... hasta n-1), y como esta desordenado ya se aplica el shuffle.\n",
    "\n",
    "\n",
    "    #ahora se tiene el resultado de la operacion modulo (y se añade como clave a cada fila)\n",
    "    indexed_data = data.map(lambda x: (indices.pop() % n_blocks, x)) \n",
    "    #'x' es una fila del RDD original, y 'indices.pop() % n_blocks' le asigna un bloque aleatorio \n",
    "    #cada bloque tiene mas o menos el mismo número de elementos\n",
    "\n",
    "    #se puede comprobar con    \n",
    "    conteo_rdd = indexed_data.map(lambda x: (x[0], 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    #agrupar por bloque y hacer cuenta del numero de elementos de ese bloque\n",
    "    for bloque, count in conteo_rdd.collect():\n",
    "        print(f\"Bloque {bloque}: {count} elementos\")\n",
    "\n",
    "    #esto lo he copiado\n",
    "    return indexed_data.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ff848f0-2038-4f27-be49-60acd5fa2c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bloque 2: 3366 elementos\n",
      "Bloque 0: 3366 elementos\n",
      "Bloque 1: 3268 elementos\n"
     ]
    }
   ],
   "source": [
    "#pruebas \n",
    "\n",
    "path = 'botnet_reduced_10k_l.csv'\n",
    "data = readFile(path)\n",
    "\n",
    "# standardize\n",
    "data = normalize(data)\n",
    "num_blocks_cv = 3\n",
    "data_cv = transform(data, data.count(), num_blocks_cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "36bb6954-5bda-4130-8e81-1ecf7190312d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(2,\n",
       "  (array([ 1.41281668, -0.75956071, -0.41204268, -0.45940748,  1.39580934,\n",
       "          -0.35386218,  0.74057161, -0.8945781 , -0.40163083, -2.91039199,\n",
       "           0.1567098 ]),\n",
       "   0)),\n",
       " (1,\n",
       "  (array([-0.79867306, -0.89146231,  3.64034447, -0.45940748, -0.52296869,\n",
       "          -0.35387136,  0.55184122, -0.8945781 , -1.27444226,  0.4762026 ,\n",
       "           0.1567098 ]),\n",
       "   1)),\n",
       " (0,\n",
       "  (array([ 1.44693571,  1.38000803, -0.41371517, -0.45940748, -0.52296783,\n",
       "          -0.35387107,  0.74057161,  1.49057358,  0.9904482 , -0.73801555,\n",
       "          -1.74828175]),\n",
       "   0))]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cv.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32d7e5e7-df69-459a-a117-02aa9ff525de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block_data(data_cv, i, n_blocks):\n",
    "    #agrupamos los datos por bloque usando el valor de la clave (n de bloque) y almacenamos los datos al lado\n",
    "    #tengo (bloque/slice , dato (array y etiqueta))\n",
    "    grouped_data = data_cv.map(lambda x: (x[0], [x[1]])) \n",
    "\n",
    "    \n",
    "    #reduceByKey' para concatenar los elementos de la misma clave (bloque), es decir, agrupamos\n",
    "    #los datos que tienen el mismo valor de bloque.\n",
    "    grouped_data = grouped_data.reduceByKey(lambda a, b: a + b) \n",
    "    #aqui tenemos clave (n de bloque, lista de datos (filas con su array de 11 elementos, etiqueta))\n",
    "    \n",
    "    #en cada iteracion de este bucle trabajo con un slice o bloque \n",
    "    #tomamos todos los elementos cuyo bloque no sea el valor 'i' actual.\n",
    "    tr_data = grouped_data.map(lambda x: (x[0], [item for item in x[1] if x[0] != i]))\n",
    "    test_data = grouped_data.map(lambda x: (x[0], [item for item in x[1] if x[0] == i])) # el resto de test (elementos con clave el i)\n",
    "\n",
    "    # hasta ahora tenia lista de listas para cada bloque (al agrupar con reducebykey)\n",
    "    #aplano y tengo solo los datos no la clave del bloque (ya no se va a usar)\n",
    "    tr_data = tr_data.flatMap(lambda x: x[1])  \n",
    "    test_data = test_data.flatMap(lambda x: x[1])  \n",
    "    \n",
    "    return tr_data, test_data\n",
    "    return tr_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83af256a-021d-47f5-be8c-5b4e8f827caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Slice 1:\n",
      "  Datos de Entrenamiento - Número de elementos: 6634\n",
      "  Datos de Prueba - Número de elementos: 3366\n",
      "Iteration [0]: [0.50299917]\n",
      "Iteration [1]: [0.36647903]\n",
      "Iteration [2]: [0.30585095]\n",
      "Iteration [3]: [0.27285125]\n",
      "Iteration [4]: [0.25227128]\n",
      "Iteration [5]: [0.23822352]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration [6]: [0.22801445]\n",
      "Iteration [7]: [0.22024882]\n",
      "Iteration [8]: [0.21413398]\n",
      "Iteration [9]: [0.20918677]\n",
      "acc : 0.9275103980986333\n",
      "\n",
      "Slice 2:\n",
      "  Datos de Entrenamiento - Número de elementos: 6732\n",
      "  Datos de Prueba - Número de elementos: 3268\n",
      "Iteration [0]: [0.74656868]\n",
      "Iteration [1]: [0.46420279]\n",
      "Iteration [2]: [0.34869523]\n",
      "Iteration [3]: [0.29532557]\n",
      "Iteration [4]: [0.26621846]\n",
      "Iteration [5]: [0.24816928]\n",
      "Iteration [6]: [0.23592747]\n",
      "Iteration [7]: [0.22707392]\n",
      "Iteration [8]: [0.2203574]\n",
      "Iteration [9]: [0.21507143]\n",
      "acc : 0.9326805385556916\n",
      "\n",
      "Slice 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Datos de Entrenamiento - Número de elementos: 6634\n",
      "  Datos de Prueba - Número de elementos: 3366\n",
      "Iteration [0]: [0.58656366]\n",
      "Iteration [1]: [0.40331198]\n",
      "Iteration [2]: [0.33140923]\n",
      "Iteration [3]: [0.29380798]\n",
      "Iteration [4]: [0.27019753]\n",
      "Iteration [5]: [0.25383388]\n",
      "Iteration [6]: [0.24177958]\n",
      "Iteration [7]: [0.23251863]\n",
      "Iteration [8]: [0.22517806]\n",
      "Iteration [9]: [0.21921595]\n",
      "acc : 0.9248366013071896\n",
      "average acc:  0.9283425126538383\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "path = 'botnet_reduced_10k_l.csv'\n",
    "data = readFile(path)\n",
    "\n",
    "# standardize\n",
    "data = normalize(data)\n",
    "\n",
    "num_blocks_cv = 3\n",
    "\n",
    "nIter = 10\n",
    "learningRate = 1.5\n",
    "lamba_reg = 0\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "for i in range(num_blocks_cv):\n",
    "    tr_data, test_data = get_block_data(data_cv, i, num_blocks_cv)    \n",
    "    print(f\"\\nSlice {i+1}:\")\n",
    "    print(f\"  Datos de Entrenamiento - Número de elementos: {tr_data.count()}\")\n",
    "    print(f\"  Datos de Prueba - Número de elementos: {test_data.count()}\")\n",
    "    #el profe en el enunciado devuelve solo ws, la funcion creada antes devuelve w, b separadamente\n",
    "    w,b = train(tr_data,nIter,learningRate, lamba_reg)\n",
    "    #el profe en el enunciado coge dos argumentos, en la funcion creada antes se cogen 3\n",
    "    acc = accuracy(w,b, test_data)\n",
    "    print(\"acc :\", acc)\n",
    "    #guardar acc de cada iteracion (que es con distinto conjunto de datos de train y test)\n",
    "    accuracies.append(acc)\n",
    "\n",
    "avg_acc = sum(accuracies) / num_blocks_cv\n",
    "print(\"average acc: \", avg_acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56710cd3-61cd-492b-a52e-0499c1a3e62b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
